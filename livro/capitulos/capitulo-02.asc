== Notação Assintótica


=== Introdução

.Objetivos do capítulo
****
Ao final deste capítulo você deverá ser capaz de:

* Entender a noção de comparação assintótica de funções
* Conhecer as principais notações utilizadas para denotar a 
relação entre a complexidade assintótica de duas funções.
* Ser capaz de identificar a classe de crescimento assintótico de 
uma determinada função utilizando a notação _O_
****

Um problema pode ter várias soluções algorítmicas. Para poder 
escolher a melhor dentre as diversas opções, é preciso ser capaz 
de julgar quanto tempo cada solução precisa para resolver o 
problema. Mais precisamente, você não precisa saber o tempo de 
execução de uma solução em minutos ou segundos mas deve ser capaz 
de, dadas duas soluções, decidir qual delas é a mais rápida.

_Complexidade assintótica_ é uma forma de expressar o componente 
principal relacionado ao tempo de execução de um algoritmo 
utilizando unidades abstratas de trabalho computacional. 

Considere, por exemplo, um algoritmo para ordenar uma pilha de cartas 
de baralho que percorre repetidas vezes a pilha a procura da menor 
carta. A complexidade assintótica deste algoritmo é o _quadrado_ do 
número de cartas na pilha. Este componente quadrático é o termo 
principal da fórmula que conta quantas operações o algoritmo 
precisa para ordenar a pilha de cartas e ele mostra, por exemplo, que 
se dobramos o tamanho da pilha, o trabalho necessário para 
ordená-la é quadruplicado. 

A formula exata para o custo de execução do algoritmo é um pouco 
mais complexa e contém detalhes adicionais necessários para 
entender a complexidade do algoritmo. Em relação ao problema de 
ordenar a pilha e cartas, o pior caso acontece quando a pilha se 
encontra em ordem decrescente pois nessa situação a busca pela 
menor carta sempre teria que ir até o final da pilha. A primeira 
busca iria varrer as 52 cartas do baralho, a próxima varreria as 
demais 51, a seguinte 50 e assim por diante. Portanto, a formula para 
o custo de execução seria _52 + 51 + 50 + ... + 2_. Sendo _N_ o 
número de cartas, a formula geral é _2 + ... + N - 1 + N_, o que é 
igual a _((1 + N) x N)/2) - 1_ = _((N^2^ + N)/2) - 1_ = _(1/2)N^2^ + 
(1/2)N - 1_.  Note que o termo _N^2^_ domina a expressão, e esta é 
a chave para comparar o custo de dois algoritmos. 

Assintóticamente falando, quando _N_ tende ao infinito, a soma _2 + 
3 + ... + N_ se aproxima cada vez mais da função puramente 
quadrática _(1/2)N^2^_.  Além disso, com um valor de _N_ muito 
grande, o fator constante _1/2_ pode ser desprezado. Com isso, 
podemos dizer que o algoritmo tem complexidade _O(n^2^)_.

NOTE: O algoritmo de ordenação descrito acima é muito ineficiente. 
Veremos no decorrer do livro algoritmos bem mais eficientes que este, 
com tempos de execução sub-quadráticos.

Considere agora a situação em que é necessário comparar a 
complexidade de dois algoritmos. Seja _f(n)_ o custo, no pior caso, 
de um dos algoritmos, expresso como uma função do tamanho da 
entrada _n_, e _g(n)_ a função que descreve o custo do outro 
algoritmo.  Por exemplo, considerando dois algoritmos de ordenação, 
_f(10)_ e _g(10)_ seriam as quantidades máximas de passos que cada 
um dos algoritmos precisaria para ordenar uma lista com 10 itens.  

Se para todos os valores de _n_ ≥ 0, _f(n)_ ≤ _g(n)_, o algoritmo 
com função de complexidade _f_ é estritamente mais rápido. 

Porém, de forma geral, nossa preocupação em termos de custo 
computacional está mais focada nos casos onde _n_ é muito grande; 
portanto, a comparação entre _f(n)_ e _g(n)_ para valores pequenos 
de _n_ é menos significativa do que a comparação entre _f(n)_ e 
_g(n)_ para _n_ maior que um determinado limiar.

NOTE: Estamos discutindo limites para o desempenho de algoritmos ao 
invés de medir a velocidade exata de cada um. O número efetivo de 
passos necessários para ordenar a nossa pilha de cartas (com nosso 
algoritmo simplório) depende diretamente da ordem em que as cartas 
se encontram inicialmente. O tempo necessário para executar cada um 
dos passos depende diretamente da velocidade do processador, do 
conteúdo da memória cache e de vários outros fatores. 

=== A notação Big-O
A notação  _O_ é o método formal utilizado para expressar o 
_limite superior_ no tempo de execução de um algoritmo. É uma 
medida da maior quantidade de tempo que um determinado algoritmo pode 
precisar para completar sua tarefa.

De maneira mais formal, para duas funções não-negativas, _f(n)_ e 
_g(n)_, se existe um inteiro _n~0~_ e uma constante _c_ > 0 tal que 
para todos os inteiros _n_ > _n~0~_, _f(n)_ ≤ _cg(n)_, então 
_f(n)_ tem complexidade _big oh_ _g(n)_, denotada por _f(n) = 
O(g(n))_. 

No gráfico a seguir é possível observar visualmente o 
comportamento das funções _f_ e _g_. Note que para _n_ > _n~0~_, 
_f(n)_ ≤ _cg(n)_.

.Notação _O_
image::imagens/capitulo-02/notacao-bigO.gif[scaledwidth="60%"]

.Funções com complexidade _O(n^2^)_
==========================
_f(n) = n_	
	
_f(n) = n/1000_

_f(n) = n^1.9^_

_f(n) = n^2^_

_f(n) = n^2^+n_

_f(n) = 1000n^2^ + 50n_
==========================

.Funções com complexidade superior a _O(n^2^)_
==========================
_f(n) = n^3^_

_f(n) = n^2.1^_

_f(n) = 2^n^_
==========================


.Complexidade de alguns algoritmos
==========================
Imprimir uma lista de _n_ itens na tela, examinando cada um deles uma 
única vez = _O(n)_

Receber uma lista de _n_ elementos e dividi-la ao meio sucessivas 
vezes até que reste um único elemento = _O(log n)_

Receber uma lista de _n_ elementos e comparar cada elemento com todos 
os outros elementos = _O(n^2^)_
==========================


.Mostre que _2n + 8 = O(n^2^)_
==========================
Sejam _f(n) = 2n + 8_ e _g(n) = n^2^_. Podemos encontrar uma 
constante _n~0~_ tal que _2n + 8 ≤ n^2^_ ?  

Analisando as funções podemos ver que para _n~0~ ≥ 4_ , _2n + 8 ≤ n^2^_. 

Uma vez que queremos generalizar essa expressão para valores de _n_ 
grandes, e valores pequenos para _n_ (1, 2 e 3) não são 
importantes, pode-se dizer que _f(n)_ é mais rápida que _g(n)_, ou 
seja, _f(n)_ é limitada por _g(n)_. Portanto, _f(n) = O(n^2^)_.

Existem alguns atalhos para encontrar o limite superior de uma 
função. Por exemplo, podemos remover todas as constantes da 
expressão pois, eventualmente, para algum valor _c_, elas se tornam 
irrelevantes. Isso faz _f(n) = 2n_. Além disso, para conveniência 
na comparação, podemos também remover constantes multiplicativas, 
neste caso, o 2, fazendo _f(n) = n_. Com isso, podemos dizer que 
_f(n) = O(n)_, e isso nos dá um limite mais _apertado_ para _f_.
==========================

==== Exercícios
1. Faz sentido dizer que _f(n) = O(n^2^)_ para _n ≥ 4_?
2. É verdade que  _10n = O(n)_ ?   
3. É verdade que  _10n^55^ = O(2^n)_ ?
4. É verdade que  _n^2^ + 200n + 300  =  O(n^2^)_ ? 
5. É verdade que  _n^2^ − 200n − 300  = O(n)_ ?
6. É verdade que  _(3/2)n^2^ + (7/2)n − 4 = O(n)_ ?
7. É verdade que  _(3/2)n^2^ + (7/2)n −  4 = O(n^2^)_ ?
8. É verdade que  _n^3^ − 999999n^2^ − 1000000 = O(n^2^)_ ?
9. Prove que _n = O(2^n^)_.  
10. Prove que _n = O(2^n/4^)_. 

TIP: Nas questões 9 e 10, use indução matemática para provar que 
_n ≤ 2^n^_ e que _n ≤ 2^n/4^_ para todo n suficientemente grande.


NOTE: Os algoritmos e estruturas de dados apresentadas neste livro 
terão sua complexidade temporal e espacial apresentadas utilizando a 
notação _O_. Porém, existem várias outras notações utilizadas 
para descrever complexidade assintótica. A seguir veremos algumas 
dessas notações e o seu significado.

=== Notação Big-Omega

A notação latexmath:[$\Omega$] (_omega_) é utilizada para 
expressar o _limite inferior_ no tempo de execução de um algoritmo. 
É uma medida da menor quantidade de tempo que um determinado 
algoritmo pode precisar para completar sua tarefa.

Para duas funções não negativas, _f(n)_ e _g(n)_, se existe um 
inteiro _n~0~_ e uma constante _c > 0_ tal que para todos os inteiros 
_n ≥ n~0~_, _f(n) ≥  cg(n)_, _f(n)_ tem complexidade _omega_ 
_g(n)_, denotado por _f(n) = latexmath:[$\Omega$](g(n))_.

Esta é praticamente a mesma definição para a notação _O_, com a 
exceção de que _f(n) ≥ cg(n)_ implica em dizer que _g(n)_ 
representa o _melhor caso_ para _f(n)_, como ilustrado na figura 
abaixo. Neste caso, _g(n)_ descreve o _melhor_ que pode acontecer 
para um determinado tamanho de entrada.

.Notação _latexmath:[$\Omega$]_
image::imagens/capitulo-02/notacao-bigOmega.gif[scaledwidth="60%"]

.Funções com complexidade _latexmath:[$\Omega$](n)_
==========================
_f(n) = n_	
	
_f(n) = n/1000_

_f(n) = n^1.9^_

_f(n) = n^2^_

_f(n) = n^2^+n_

_f(n) = 1000n^2^ + 50n_
==========================

.Funções com complexidade inferior a _latexmath:[$\Omega$](n^2^)_
==========================
_f(n) = n_

_f(n) = n^1.9^_

_f(n) = log(n)_
==========================

==== Exercícios

1. Prove que _100 lg n − 10n + 2n lg n = latexmath:[$\Omega$](n lg n)_.  
2. É verdade que _2^n+1^ = latexmath:[$\Omega$](2^n)_ ?
3. É verdade que _3^n^ = latexmath:[$\Omega$](2^n^)_ ?


=== Notação Theta

A notação latexmath:[$\Theta$] (_theta_) é utilizada para definir 
um _limite assintótico restrito_ para o tempo de execução de um 
algoritmo. É uma medida que define tanto limites superiores quanto 
inferiores para o crescimento da função _f(n)_.

Sejam duas funções não-negativas _f(n)_ e _g(n)_, _f(n)_ é dita 
theta de _g(n)_ se e somente se _f(n) = O(g(n))_ e _f(n) = 
latexmath:[$\Omega$](g(n))_, denotado por _f(n) = 
latexmath:[$\Theta$](g(n))_.

Isto significa que a função _f(n)_ é limitada tanto superior 
quanto inferiormente, pela função _g(n)_, conforme ilustrado na 
figura abaixo.

.Notação _latexmath:[$\Theta$]_
image::imagens/capitulo-02/notacao-theta.gif[scaledwidth="60%"]


==== Exercícios

1. É verdade que  _(3/2)n^2^ + (7/2)n^3^ − 4 = latexmath:[$\Theta$](n^2^)_ ?
2. É verdade que _9999n^2^ = latexmath:[$\Theta$](n^2^)_ ?
3. É verdade que _n^2^/1000 − 999n = latexmath:[$\Theta$](n^2^)_ ?
4. É verdade que _log~2~n + 1 = latexmath:[$\Theta$](log~10~n)_ ?


=== Outras notações

==== Notação Little-o

A notação _o_ representa uma versão mais solta da notação _O_. 
Nesta notação, _g(n)_ limita a função _f(n)_ superiormente mas 
não representa um limite assintótico restrito.

Sejam duas funções não-negativas, _f(n)_ e _g(n)_, _f(n)_ é dita 
_little o_ de _g(n)_ se e somente se _f(n) = O(g(n))_ mas _f(n) ≠ 
latexmath:[$\Theta$](g(n))_, denotando-se por _f(n) = o(g(n))_.

==== Notação Little-omega

A notação _latexmath:[$\omega$]_ (_pronunciada little omega_) 
representa uma versão mais solta da notação 
_latexmath:[$\Omega$]_. Nesta notação _g(n)_ limita a função 
_f(n)_ inferiormente mas não representa um limite assintótico 
restrito.

Sejam duas funções não-negativas, _f(n)_ e _g(n)_, _f(n)_ é dita 
_little omega_ de _g(n)_ se e somente se _f(n) = 
latexmath:[$\Omega$](g(n))_ mas _f(n) ≠ 
latexmath:[$\Theta$](g(n))_, denotando-se por _f(n) = 
latexmath:[$\omega$](g(n))_.

=== Notação Assintótica e Análise de Complexidade

Tempo de execução não é a única métrica de interesse ao 
analisarmos a complexidade de um algoritmo. Existem também fatores 
de interesse relacionados ao espaço de memória utilizado. 
Geralmente, há uma relação direta entre tempo de execução e 
quantidade de memória utilizada. Se pensarmos na quantidade de tempo 
e espaço que um programa utiliza como uma função do tamanho da 
entrada podemos analisar como estas métricas evoluem quando mais 
dados são introduzidos no programa.

Isto é particularmente importante no projeto de estruturas de dados 
pois geralmente desejamos utilizar uma estrutura que se comporte de 
forma eficiente quando grandes volume dados são fornecidos. 
Entretanto, é importante compreender que nem sempre algoritmos que 
são eficientes para grandes volumes de dados são simples e 
eficientes para pequenos volumes dados.

A notação assintótica é geralmente utilizada como uma forma 
conveniente de expressar o que acontece com uma função no pior e no 
melhor caso. Por exemplo, se você deseja escrever uma função que 
pesquisa um arranjo de números em busca do menor valor:

.Código para encontrar o menor elemento de um arranjo
[source,c,numbered]
----
int findMin(int array[], int size) {
	int min = MAX_INT;
	int i;
	for(i = 0; i < size; i++) {
		if(array[i] < min)
			min = array[i];
	}
	return min;
}
----

Independente do quão grande seja o arranjo, sempre que a função 
_findMin()_ é executada é preciso inicializar as variáveis _min_ e 
_i_ e retornar a variável _min_ ao fim da execução. Portanto, 
podemos considerar estes trechos do código como constantes e 
ignorá-los no cálculo da complexidade.

Então, como podemos utilizar a notação assintótica para descrever 
a complexidade da função _findMin()_ ? Se a função for utilizada 
para encontrar o valor mínimo de um arranjo com 87 elementos o _for_ 
realizará 87 iterações, mesmo que o menor elemento seja o primeiro 
a ser visitado. De maneira análoga, para _n_ elementos o _for_ 
realizará _n_ iterações. Portanto, podemos dizer que a função 
_findMin()_ tem tempo de execução _O(n)_.

O que dizer agora acerca da função abaixo:

.Código para encontrar a soma do menor e maior elementos de um 
arranjo
[source,c,numbered]
----
int findMinPlusMax(int array[], int size) {
	int min = MAX_INT;
	int max = MIN_INT;
	int i;

	//Encontra o valor mínimo no arranjo
	for(i = 0; i < size; i++) {
		if(array[i] < min)
			min = array[i];
	}

	//Encontra o valor máximo no arranjo
	for(i = 0; i < size; i++) {
		if(array[i] > max)
			max = array[i];
	}

	return min + max;
}
----

Qual função melhor representa o tempo de execução da função 
_findMinPlusMax()_?  Existem dois laços e cada um realiza _n_ 
iterações sendo assim o tempo de execução é claramente _O(2n)_. 
Uma vez que 2 é uma constante ele pode ser ignorado e o tempo de 
execução da função pode ser descrito como _O(n)_.

=== Recaptulando

Neste capítulo vimos uma noção sobre a comparação assintótica 
de funções e conhecemos as principais notações utilizadas para 
expressar o relacionamento assintótico entre duas funções.

Uma analogia imprecisa entre a comparação assintótica de duas 
funções _f_ e _g_ e a relação entre seus valores é dada a seguir:

_f(n) = O(g(n))_  ≈  _f(n) ≤ g(n)_

_f(n) = latexmath:[$\Omega$](g(n))_  ≈  _f(n) ≥ g(n)_

_f(n) = latexmath:[$\Theta$](g(n))_  ≈  _f(n) = g(n)_

_f(n) = o(g(n))_  ≈  _f(n) < g(n)_

_f(n) = latexmath:[$\omega$](g(n))_  ≈ _f(n) > g(n)_


Estas notações, particularmente a notação _O_, serão utilizadas 
no decorrer deste livro para expressar complexidade temporal de 
algoritmos e complexidade espacial de estruturas de dados.

No próximo capítulo iniciamos efetivamente o estudo de estruturas 
de dados abordando os arranjos (_arrays_) unidimensionais, também 
denotados _vetores_ e arranjos multi-dimensionais,  que podem ser 
denominados de _matrizes_ no caso de termos apenas duas dimensões.

// Sempre terminar o arquivo com uma nova linha.

